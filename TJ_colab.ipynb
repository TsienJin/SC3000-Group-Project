{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyNTnqpGvQdibGlaxV7nK1sg",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TsienJin/SC3000-Group-Project/blob/main/TJ_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OrS4hCnwGXVD"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deepcopy\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import matplotlib\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "Observation = namedtuple(\"observation\", (\"cartPos\", \"cartVel\", \"poleAngle\", \"poleVel\"))\n",
    "\n",
    "Environment = namedtuple(\"environment\", (\"observation\", \"reward\", \"isDone\", \"isTruncated\"))\n",
    "\n",
    "# Referenced as \"experience\" in the DQN paper\n",
    "Record = namedtuple(\"record\", (\"state\", \"action\", \"nextState\", \"reward\"))\n"
   ],
   "metadata": {
    "id": "x2mdmOmtG2lk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class ParseEnvironment:\n",
    "    def __init__(self, environment:[float], reward:float=None, isDone:bool=None, isTruncated:bool=None, *args):\n",
    "        self.cartPos = environment[0]\n",
    "        self.cartVel = environment[1]\n",
    "        self.poleAngle = environment[2]\n",
    "        self.poleVel = environment[3]\n",
    "\n",
    "        self.reward = reward\n",
    "        self.isDone = isDone\n",
    "        self.isTruncated = isTruncated\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"cPOS: {self.cartPos}\\ncVEL: {self.cartVel}\\npANG: {self.poleAngle}\\npVEL: {self.poleVel}\\nreward: {self.reward}\\nisDone: {self.isDone}\\nisTruncated: {self.isTruncated}\"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "    def toObservation(self) -> Observation:\n",
    "        return Observation(self.cartPos, self.cartVel, self.poleAngle, self.poleVel)\n",
    "\n",
    "    def toTensor(self):\n",
    "        return torch.FloatTensor((self.cartPos, self.cartVel, self.poleAngle, self.poleVel))\n",
    "\n",
    "    def toFloat32(self) -> [np.float32]:\n",
    "        return np.array([self.cartPos, self.cartVel, self.poleAngle, self.poleVel], type=np.float32)\n",
    "\n",
    "    def toEnvironment(self) -> Environment:\n",
    "        return Environment(self.toObservation(), self.reward, self.isDone, self.isTruncated)\n",
    "\n",
    "\n",
    "class ParseRecord:\n",
    "    def __init__(self, state: ParseEnvironment, action: int, nextState: ParseEnvironment, reward: float):\n",
    "        assert action in [0, 1]\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.nextState = nextState\n",
    "        self.reward = reward\n",
    "\n",
    "    def toRecord(self) -> Record:\n",
    "        return Record(self.state, self.action, self.nextState, self.reward)\n"
   ],
   "metadata": {
    "id": "lMli804DHJlN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class Memory:\n",
    "    def __init__(self, maxCapacity:int=10000):\n",
    "        self.cap = maxCapacity\n",
    "        self.memory = deque([], maxlen=maxCapacity)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.memory)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"\"\"Memory() capacity [{self.__len__}/{self.cap}]\"\"\"\n",
    "\n",
    "    def push(self, record:ParseRecord) -> None:\n",
    "        self.memory.append(record)\n",
    "\n",
    "    def sample(self, size:int) -> [ParseRecord]:\n",
    "        assert size>0\n",
    "        return random.sample(self.memory, size)\n"
   ],
   "metadata": {
    "id": "cZ5YslyxG5eQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_obsv: int, n_actions: int, n_layer: int = 1, n_layerSize: int = 6,\n",
    "                 learningRate: float = 0.0001, gamma: float = 0.95,\n",
    "                 expDecay: float = 0.999, expMin: float = 0.001, expMax: float = 1.0,\n",
    "                 _device: str = \"cpu\",\n",
    "                 memory: Memory = Memory()):\n",
    "        \"\"\"\n",
    "\n",
    "        :param n_obsv: size of observation space\n",
    "        :param n_actions: size of action space\n",
    "        :param n_layer: number of hidden layers\n",
    "        :param n_layerSize: number of neurons per hidden layer\n",
    "        :param learningRate:\n",
    "        :param gamma: discount for future values\n",
    "        :param expDecay:\n",
    "        :param expMin:\n",
    "        :param expMax:\n",
    "        :param _device: defaults to \"cpu\"\n",
    "        \"\"\"\n",
    "\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Ensuring that values are proper\n",
    "        assert n_layer >= 0\n",
    "        assert n_obsv > 0\n",
    "        assert n_actions > 0\n",
    "        assert n_layerSize > 0\n",
    "        assert 0 < learningRate < 1\n",
    "        assert 0 < gamma < 1\n",
    "        assert 0 < expDecay < 1\n",
    "        assert 0 < expMin < 1\n",
    "        assert 0 < expMax <= 1\n",
    "\n",
    "        self.learningRate = learningRate\n",
    "        self.gamma = gamma\n",
    "        self.expDecay = expDecay\n",
    "        self.expMin = expMin\n",
    "        self.expMax = expMax\n",
    "\n",
    "        self.n_obsv = n_obsv\n",
    "        self.n_actions = n_actions\n",
    "        self.n_layer = n_layer\n",
    "        self.n_layerSize = n_layerSize\n",
    "\n",
    "        self.memory = memory\n",
    "\n",
    "        self.layers = nn.ModuleList(self.__createLayers())\n",
    "        self.optim = optim.Adam(self.parameters(), lr=self.learningRate)\n",
    "        self.crit = torch.nn.SmoothL1Loss()  # Huber loss\n",
    "\n",
    "        self.to(_device)\n",
    "\n",
    "    def __createLayers(self):\n",
    "        \"\"\"\n",
    "        Private method to generate neural network given the specified params in __init__()\n",
    "        :return: [nn.Linear()]\n",
    "        \"\"\"\n",
    "        # init layers starting with input shape to layer size\n",
    "        layers = [nn.Linear(self.n_obsv, self.n_layerSize)]\n",
    "\n",
    "        # creates more layers with specified layer size\n",
    "        for _ in range(self.n_layer):\n",
    "            layers.append(nn.Linear(self.n_layerSize, self.n_layerSize))\n",
    "\n",
    "        # adds final output layer\n",
    "        layers.append(nn.Linear(self.n_layerSize, self.n_actions))\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x:torch.float32) -> torch.float32:\n",
    "        \"\"\"\n",
    "        Processes the given state and returns a tensor with qValues for actions\n",
    "        :param x: <torch.tensor> with shape (1,4) and type float32 | State of current observation as a tensor\n",
    "        :return: <torch.tensor> with shape (1,2) | Tensor of qValues\n",
    "        \"\"\"\n",
    "        assert (x.dim() == torch.randn(4).dim())\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return x"
   ],
   "metadata": {
    "id": "AiW2eKE1HFk_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class Agent:\n",
    "    # Number of episodes\n",
    "    MAX_EP = 1_000_000\n",
    "\n",
    "    # Q Value vals\n",
    "    DISCOUNT = 0.9\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    # Epsilon GREEDY vals\n",
    "    EPS = 0.9999\n",
    "    EPS_DECAY = 0.999\n",
    "    EPS_MIN = 0.05\n",
    "    EPS_MAX = 1.0\n",
    "\n",
    "    # Memory vals\n",
    "    MEM_SIZE = 50_000\n",
    "    MIN_MEM_SIZE = 1_000\n",
    "    MEM_BATCH = 200\n",
    "    TARGET_UPDATE_FREQ = 75\n",
    "\n",
    "    def __init__(self, maxEp:int=10_000, env=gym.make(\"CartPole-v1\")):\n",
    "\n",
    "        # Bootstrapping to maintain stability of prediction\n",
    "        self.memory = Memory(maxCapacity=self.MEM_SIZE)\n",
    "        self.model = DQN(n_obsv=4, n_actions=2, n_layer=10, n_layerSize=10,learningRate=self.LEARNING_RATE, memory=self.memory)  # updates every iteration\n",
    "        self.targetModel = deepcopy(self.model)  # updates only once threshold has been reached\n",
    "\n",
    "        # Setting individual stats for the environment to run\n",
    "        self.maxEpisode = maxEp\n",
    "        self.env = env\n",
    "        self.episodeCounter = 0\n",
    "        self.totalReward = 0\n",
    "\n",
    "    def __printStats(self):\n",
    "        print(f\"EP: {self.EPS:.3f} | MEM: {len(self.memory)} | EP: {self.episodeCounter} | AVG: {self.totalReward/self.episodeCounter:.5f}\")\n",
    "\n",
    "    def predict(self, environment:ParseEnvironment) -> int:\n",
    "        if self.EPS < self.EPS_MIN:\n",
    "            res = self.targetModel.forward(environment.toTensor())\n",
    "            return torch.argmax(res).detach().numpy()\n",
    "        else:\n",
    "            # print(\"USING RANDOM\")\n",
    "            self.EPS = self.EPS * self.EPS_DECAY\n",
    "            return random.randint(0,1)\n",
    "\n",
    "\n",
    "    def getMaxQ(self, environment:ParseEnvironment) -> torch.tensor:\n",
    "        res = self.targetModel.forward(environment.toTensor())\n",
    "        return res.clone().detach().numpy()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < self.MIN_MEM_SIZE:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(size=self.MEM_BATCH)\n",
    "\n",
    "        allStates = np.array([record.state for record in batch])  # need to check if this works; no intellisense\n",
    "        predicted = [self.getMaxQ(record.state) for record in batch]\n",
    "        predictedNew = [self.getMaxQ(record.nextState) for record in batch]\n",
    "\n",
    "        oldValsToFit = []\n",
    "        valsToFit = []\n",
    "\n",
    "        for index, env in enumerate(batch):\n",
    "            maxFutureQ = np.max(self.getMaxQ(env.nextState))\n",
    "\n",
    "            if not env.state.isDone:\n",
    "                newQ = env.reward + self.DISCOUNT * maxFutureQ\n",
    "            else:\n",
    "                newQ = env.reward\n",
    "\n",
    "            oldFit = self.getMaxQ(env.state)\n",
    "            toFit = deepcopy(oldFit)\n",
    "            toFit[env.action] = (1-self.LEARNING_RATE)*oldFit[env.action] + self.LEARNING_RATE * newQ\n",
    "\n",
    "\n",
    "            oldValsToFit.append(oldFit)\n",
    "            valsToFit.append(toFit)\n",
    "\n",
    "        loss = self.model.crit(torch.tensor(np.array(oldValsToFit), requires_grad=True), torch.tensor(np.array(valsToFit), requires_grad=True))\n",
    "        self.model.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.model.optim.step()\n",
    "\n",
    "        if self.episodeCounter % self.TARGET_UPDATE_FREQ == 0:\n",
    "            self.targetModel.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        while self.episodeCounter < self.maxEpisode:\n",
    "            self.episodeCounter += 1\n",
    "            cReward = 0.0\n",
    "            curEnv = ParseEnvironment(self.env.reset()[0], reward=1.0, isDone=False, isTruncated=False)\n",
    "\n",
    "            while curEnv.isDone is not True:\n",
    "\n",
    "                # interact with env\n",
    "                action = self.predict(curEnv)\n",
    "                prevEnv = curEnv\n",
    "                curEnv = ParseEnvironment(*self.env.step(action))\n",
    "\n",
    "                # save record of what just happened\n",
    "                thisRecord = ParseRecord(prevEnv, action, curEnv, curEnv.reward)\n",
    "                self.memory.push(thisRecord)\n",
    "\n",
    "                # # train model\n",
    "                self.train()\n",
    "\n",
    "                # # update local variables\n",
    "                cReward += curEnv.reward\n",
    "                self.totalReward += curEnv.reward\n",
    "                self.__printStats()\n"
   ],
   "metadata": {
    "id": "hfM8SLMJHSnU",
    "outputId": "d9aac268-d00d-4e69-9713-e886de80f224",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "agent = Agent(maxEp=1000)\n",
    "agent.run()"
   ],
   "metadata": {
    "id": "pE6jOKKEHdXb",
    "outputId": "c8813492-ae85-41da-bca8-efed4942379e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "K5sGUAI3Hfe7"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
